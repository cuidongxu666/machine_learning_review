一个是深入理解算法基本原理总结、自建算法、sklearn应用
机器学习前言
    规则很难定义，所以用机器学习
        1.深入理解算法原理（算法底层编写）
        2.解决实际问题
        3.算法对比试验
        4.超参数对比试验（调参重点）

        重要任务（如何使用算法）：
            评价算法好坏
            解决过拟合欠拟合
            调参
            验证算法正确性

    不好地方：只会调库，却对库没有深入理解；如：什么情况下使用哪种排序；理解算法才能选择算法

    anaconda
    编程环境（IDE）：jupyter notebook，pycharm

    本课程入门，不包含视觉、推荐系统、自然语言处理、时间序列分析；数据繁琐预处理不介绍

    人工智能包含
        搜索算法如梯度下降
        机器学习算法里包含深度学习
        神经网络也是机器学习中重要算法，是深度学习的基础（本课程不涉及）

    数学上向量表示为列向量，在搞成行向量；例如一个样本的特征表示为列向量

    机器学习，通过大量数据得到模型（函数），样例--》结果
    机器学习方法的分类：
        监督学习：数据拥有标记
        非监督学习：数据无标记，作用：对数据分类（聚类）；数据降维（特征提取，特征压缩（PCA）），方便可视化；异常检测
        半监督学习：部分有标记，部分没有标记；非监督学习处理+监督学习训练预测
        增强学习：（机器人，无人驾驶）；监督学习与半监督学习是其基础
    机器学习其他分类：
        批量学习（离线学习，本课程重点批量学习）：模型不会发生变化，不会优化模型
            简单，但无法适应环境变化 解决方法：定时重新批量学习（但计算量巨大）；环境变化太快不能用（如股市）
        在线学习：样例、结果、正确结果变学习资料，迭代；还在不断训练模型；
            及时反映环境变化，但是新的数据可能带来不好的变化；解决方法：对数据加强监控（异常检测）
        参数学习：学到参数，原有数据集就没用了
        非参数学习：也有参数，只是不对问题进行建模；

    脱离具体问题，谈算法好坏是没有意义的；
    在具体问题前，需要尝试多种算法进行对比试验，是必要的


机器学习算法：

K近邻算法
    定义：样例，在数据集中寻找距离最近的k个样本，以k个样本的标记进行投票分类
    列表变数组np.array(list)
    距离：
        2欧拉距离：对应差值平方和，开平方
        1曼哈顿距离：对应差值绝对值之和
        p明可夫斯基距离
    KNN算法不需要训练，没有模型；但是为了统一，认为数据集就是模型本身

    判断算法性能
    预测模型好坏：测试数据集

    超参数与模型参数
    超参数：算法运行前需要确定的参数
    模型参数：算法过程中学习的参数

    普通KNN投票，k个投票，k个点所属类别的个数最多，没考虑距离
    考虑距离权重（各种类距离倒数和比较），距离近的权重大
    考虑距离解决平票情况

    KNN超参数：k,weight,距离p等等
    weight：uniform（不考虑权重），distance（考虑）
    不考虑距离权重就不用考虑距离p


    数据归一化：特征量纲不同，将所有的数据映射到同一 尺度
        最值归一化：映射到0~1；适用于有明显边界的情况，受异常点影响较大
        均值方差归一化（标准化 ）：把数据归一到均值为0方差1的分布中；适用于没有明显边界或极端数据值
        测试数据集模拟真实环境，可能无法测得均值方差，测试数据集的归一化 使用训练集的均值方差
        fit保存均值方差，transform（相当于predict ）


    KNN总结：
        解决多分类
        思想简单，效果强大
        knn解决回归问题，取最近k点，平均值；或考虑距离权重，加权平均

        缺点：效率低下，m样本，n特征，预测一个新数据O（m*n）
             高度数据相关，可能预测错误
             预测结果不具有可解释性
             维数灾难，随着维度增加，两点距离越来越大



线性回归算法
    线性回归是多项式回归、逻辑回归、SVM的基础，他们是线性回归的拓展

    简单线性回归（特征只有一个）
    y=ax+b
    误差 |真实值-预测值|，误差最小，求得ab
    绝对值，不是处处可导，用差的平方  和（也在减少 最大误差的差距  ）
    目标函数：损失函数，效用函数；求目标函数最小值时确定参数

    最优化原理、凸优化
    典型最小二乘法问题：最小化误差的平方和 ，通过最小二乘法求得a，b
                     对目标函数分别对a，b求偏导=0，求得a，b
    向量化运算： 和(··)(··)向量点乘（对应元素乘积之和）；向量化运算比for循环快许多；
    向量在数组中表示为一维数组，在python中，只要是向量都能点乘
    数组中取出某一列或某一行都是向量
    回归算法的评价：
        分类算法，可以用准确度
        目标函数针对训练集，得到模型参数
        得到模型后，测试集带入，得到预测值，与测试集真实值做对比
            均方误差MSE sum(（yt-yt）^2)/m,m为样本；除以m排除样本数量的影响
            均方根误差RMSE，解决量纲问题，不然是平方
            平均绝对误差MAE sum|y-y|/m,预测的平均误差在多少左右
            RMSE>MAE,前者放大误差，后者没有放大；所以尽量让RMSE值小，意义更大，意味着样本中最大错误比较小
        上面评价的标准的劣势，无法像分类的准确度一样（分类的问题不同，能比较算法作用于某个问题更好 ）
            （如房价和学生成绩）上面回归评价标准无法比较 算法更适用于哪个问题
        引入R方，1- （（预测-真值）平方和）/（（真值均值-真值）平方和））
                1-MSE/VAR

        分类中score返回准确率，线性回归中score直接返回R2


    多元线性回归
      目标函数与简单线性回归一样
      同最小二乘法原理一样，求导=0，求出参数，正规方程解，时间复杂度高O（n^3）,优点：不要归一化处理

    可解释性：
        系数正相关、负相关，系数绝对值大小决定影响程度
    线性回归总结
        评价线性回归算法：R2
        只能解决回归问题
        解释性



梯度下降法（代码放在线性回归那章）
       一种搜索方法，作用：最小化一个损失函数
       梯度上升法：最大化一个效用函数
       导数代表损失函数增大的方向
       学习率：影响获得最优解的速度；是梯度下降法的超参数
       局部最优解，解决方案：多次运行，随机化初始点，初始点也是超参数
        python里正无穷-正无穷=nan
       线性回归法的损失函数具有唯一最优解

    多元线性回归中的梯度下降法  先求梯度（向量），再下降
    梯度代表方向，对应J增大最快的方向；梯度是分别对theta求偏导，组成列向量
    目标函数本来是误差平方之和
        目标函数除以m，除去样本数目对梯度的影响；再求梯度；也就是说用梯度下降法时，目标函数变成了训练集的均方误差

    向量化（求梯度的过程）
    np里是不区分行列向量的

    在梯度下降之前，进行数据归一化
    特征越多，梯度下降比正规方程的优势越大
    批量梯度下降法
        当计算梯度时，每一个样本都会参与计算；所以，样本量较大时，计算梯度也比较慢
        优点：稳定，梯度方向是损失函数下降最快的速度

    随机梯度下降法
    随机取一个样本计算出梯度向量 ；学习率变得很重要，学习率固定，可能跳出J最小值；所以学习率是递减的，随着循环次数增加而减少
    学习率=a/(次数+b)，分母b保证学习率变化太大 ，b是随机梯度下降的超参数
    随机梯度不需要J损失函数，批量梯度需要
    计算快，但不稳定，每一次的下降的方向不确定
    跳出局部最优解

    关于梯度的调试

    小批量梯度下降法
    每次不看所有样本，也不看一个样本，看k个样本



多项式回归（使用线性回归思路 ）（升维）
    sklearn没有提供这个接口
    改造 特征x2等，进行线性拟合，画散点图时记得排序
    了解特征的形成，在preprocessing模块，degree=2，degree=3

    多项式回归流程
    pipeline（管道，实例化，参数为列表，列表里为元组，（名字+实例化））
        多项式特征
        归一化
        线性回归、岭回归

    过拟合与欠拟合
    多项式degree取值大，模型越复杂，越容易过捏合；对于knn，k越小，模型越复杂
    模型泛化能力
    欠过可视化：学习曲线（随着样本的逐渐增多，算法训练出的模型表现能力）
    测试数据集的意义，我们在想办法找到一组参数，使得训练数据集获得的模型在测试数据集上效果最好
    产生问题：针对特定测试数据集过拟合
    解决：
        训练集，验证集（创建模型，验证集调整超参数）
        测试集（衡量最终模型性能的数据集）

    k-folds交叉验证 ：训练集分成k份，分别做验证集，其余做训练集，k个模型的均值作为参照结果
        缺点：每次训练k个模型，整体慢了k倍，但是找到的超参数搭配值得信赖
        k的极端，留一法（LOO-CV）把训练集分成m分，每个样本算一份；
            优点：完全不受随机影响，最接近正确；缺点：计算量巨大

    偏差方差平衡
    模型误差：偏差+方差+不可避免的误差
    欠拟合导致偏差
    模型复杂导致方差（过拟合）
    非参数学习是高方差
    参数学习是高偏差
    偏差方差是对立的，找一个平衡点
    机器学习主要挑战来自方差，高方差（过拟合）
        解决：降低模型复杂度
             降维，降噪
             增加样本数
             交叉验证
             模型正则化（重点）

    模型正则化：解决过拟合（方差大），增强模型泛化能力
                限制系数大小
    损失函数加上正则化项

    岭回归和LASSO回归针对多项式回归
    岭回归Ridge Regression（ 目标函数+正则化项）系数*theta平方和；
    LASSO回归（目标函数+正则化项）系数*|theta|之和
    岭回归与LASSO回归比较
        岭回归，系数增大，还是曲线，让theta变很小的值；计算准确，但是特征数量大，计算量大，此时优先选择弹性网
        LASSO，系数增大，趋向直线，使部分theta变0，表明theta对应特征是没有用的，可作为特征选择
            尽管如此，LASSO可能会把某些相关特征错误去除，得到模型偏差比较大；特征数较大，可用LASSO

    Lp范数（形式像明可夫斯基距离 ）
    Ridge L2正则项（这边没有开方）
    LASSO L1正则项
    L0正则项，让theta数目越少越好（极少用 ）
    弹性网：结合 L2正则项+L1正则项



逻辑回归（非常重要，推导也要会）
    分类（二分类）；回归（计算概率）

    sigmoid函数，值域（0，1），方便表达概率，确定分类
    问题：
        给定数据集，如何找到theta
    如何定义损失函数（目标函数）
        逻辑回归是预测概率p，分类[0,1]
        若y_true=1，p越小，cost越大
        若y_true=0，p越大，cost越大
        某样本损失函数表示：
            -log(p)   y_true=1；p小，cost越大，p->1,cost越小
            -log(1-p) y_true=0；反之
        某样本损失函数合成：
            cost=-ylog(p)-(1-y)log(1-p)
        整体损失函数: .....，无正规方程解；使用梯度下降法，这个函数为凸函数，无局部最优解，只存在全局最优解
        其中p=sigmoid(线模型方程)

    决策边界
        逻辑回归中，得出theta后，带入线性方程，公式=0时，这条线为决策边界

    不规则的决策边界的绘制方法（这边画图的代码有时间了解一下）
        每个样本分类后，得到决策边界

    逻辑回归中添加多项式特征：也就是说样本的决策边界不一定是直线，当直线不能很好的分类（现实都这样），那么对特征添加多项式特征
    但degree过大，就会过拟合，所以加入正则化项

    逻辑回归中使用正则化
    sklearn中逻辑回归系数放在J前；线性回归系数放在正则项前
    逻辑回归中默认L2，系数为 1
    二分类变多分类：OVR,ONE VS REST;n个类别进行n次分类，选择分类得分（逻辑概率）最高的
                  OVO,一对一，投票决定；n个类型进行C（n，2）次分类；比OVR消耗时间多，但更准确

    sklearn逻辑回归默认支持多分类任务OVR;
    sklearn也有OVO\OVR分类器
    from sklearn.multiclass import OneVsRestClassifier
    ovr=(二分类估计器)；ovr=fit();ovr.score()


分类算法的评价
    对于极度偏斜的数据，分类准确度是远远不够的

    混淆矩阵（主要针对二分类）偏斜数据，偏斜的做1
    精准率：预测1，实际1的数目/预测1总数
    召回率：预测1，实际1的数目/实际1总数
    混淆矩阵、精准率、召回率、F1-score、precision-recall-curve（平衡曲线 ）、ROC都在metrics中

    解读精准率和召回率
    根据使用场景确定 股票（注重精准率） 病人（召回率）
    还有一些情况希望获得两者平衡：F1-score， 精准率召回率的调和平均值（为啥用这？两者一高一低，得低；两者均高，得高）

    精准率和召回率是两个互相矛盾的指标（PR曲线）
    precision-recall平衡曲线，不同超参数得到不同模型，画平衡线，外围曲线模型比内围曲线模型好（外围pr都比内围大）
        一般情况下不用PR曲线（操）衡量模型优劣

    ROC曲线（对有偏数据并不敏感，不像PR）：主要用于比较两个模型优劣 ；面积大，模型好
    TPR(召回率)
    FPR(预测1实际0/实际0总数 )
    TPR与FPR趋势一致；
    ROC面积越大，模型越好.面积最大为1；from sklearn.metrics import roc_auc_score

    多分类问题中的混淆矩阵（暂时了解一下就行）


支撑向量机SVM（找到决策边界--离两个类别的最近样本最远，最近样本点成为支撑向量）
    某些 数据的决策边界不唯一
    决策边界的泛化能力
    SVM对决策边界泛化能力的考量没有寄托在数据的预处理或是正则化上，而是直接放在算法内部
    SVM最大化margin（支撑向量之间的距离 ）
    解决的是线性可分问题，hard margin SVM
                        soft margin SVM（容错 ）

    点到直线距离，最终退出待条件的目标函数（hard margin SVM）严格，泛化能力可能不行
    所以需要soft margin SVM，
        容错（条件公式放宽 ）每个样本都有一个对应的ζ（ζ>=0 ）;
            防止ζ太大，目标函数加上所有样本ζ和*C（相当于正则化项L1）
                L2（+ζ平方和）
    线性回归与逻辑回归C放前面，表达意思都一样，C越大，容错率越小
    涉及距离，量纲不同产生影响，所以使用svm之前数据归一化处理


    #线性SVM,决策边界是直线（数据点线性可分）
    from sklearn.svm import LinearSVC
    #非线性SVM，决策边界曲线（数据点线性不可分）
    from sklearn.svm import SVC

    SVM处理非线性数据
    1.使用多项式特征 ，pipline，是决策边界不在是直线；使用多项式特征，先变换（存取结果，空间变大），在计算；
    2.SVM多项式核函数接口
        SVC(kernel=‘poly’（核函数），degree（核函数阶数d）,C(正则项),coef0（核函数里面的小c） )

    核函数：使用核函数，替换原有数据，不需要存取 ；存取空间小
          目标函数里有xy点乘；核函数K(x,y)，把x,y向量变换后计算点乘
    多项式核函数K(x,y)=（xy+c）^d ；线性核函数d=1，c=0；两超参数


    高斯核函数（RBF）（SVM使用最多的核函数）
    K(x,y)=e^(-γ||x-y||^2)；一个超参数
    RBF核本质是将每一个样本点映射到一个无穷维的特征空间
    mn映射到mm；样本数量小于特征数，可以用；自然语言处理
    开销大
    高斯核函数与高斯函数（正太分布）形式上差不多，可以把γ理解为标准差的倒数
    γ越大，分布越窄；模型复杂度越高
    每一个样本点都形成钟形图案
    SVM多分类？？？怎么搞的
    计算概率：SVC(probability=True)
    SVM解决回归问题（暂时没看）
    分类，margin内点越少越好；回归margin范围内点越多越好（margin指定长度 ）


决策树（多分类），
非参数学习算法
from sklearn.tree import DecisionTreeClassifier（max_depth,criterion='h','g'）

决策树的深度为判别次数
问题：
    1.每个节点在哪个维度划分
    2.某个维度在哪个值划分

利用信息熵划分
信息熵：随机变量不确定度的度量
熵越大，不确定性越高，越随机
公式：H=-∑p*log（p）；系统中，有k类，每类占比p；推论？每类占比一样，信息熵最高
每个节点 基于某个维度的某个阈值 划分后，使整个系统信息熵降低（让系统更加确定）
#重点：划分后，对划分后的两组分别求信息熵，相加（划分后的整体信息熵）

利用基尼系数划分
基尼系数
公式：G=1-∑p^2
基尼系数越大，不确定性越高，越随机；与信息熵规律一样

信息熵vs基尼系数
sklearn默认基尼系数
信息熵（log）计算比基尼系数稍慢；
两者大多时候没什么差异

CART与超参数
sklearn的决策树：CART(分类与回归都行)
非参数学习算法均容易产生过拟合
训练时间复杂度：O（m*n*logm）太大；容易过拟合
预测时间复杂度：O（logm）m样本个数

剪枝：降低复杂度，解决过拟合
    减少决策树深度

决策树中的超参数：
max_dep th,
min_samples_split(每个节点最小样本数目，太高，欠拟合 )
min_samples_leaf(叶子节点最小样本数目)
max_leaf_nodes(最多叶子节点个数)

决策树分类：叶子节点投票 ，哪个类别票数多

决策树解决回归问题，超参数与分类一样
相应节点平均值

决策树的局限性
1.决策边界都是横平竖直
2.对个别样本点 敏感（如删除某点，决策边界变得完全不同 ）
    数据稍微偏斜，决策边界就不能很好的反应分类
    非参数学习算法都有此缺点，高度依赖于调参

机器学习中，用随机森林，而不是单独一颗决策树


集成学习（综合意见 ）
from sklearn.ensemble import VotingClassifier
VotingClassifier(estimators=[
('name',分类器实例),
()
]，voting='hard'\'soft ')
先把把算法超参数调整到最好，再vote
hard少数服从多数
soft，求各模型类别概率和的平均值，哪类大属于哪类
    soft要求集合的各模型都能估计概率
    逻辑回归predict_proba

上面的方法无法集成更多的模型
如何更多子模型+差异性：每个子模型只看样本一部分（子模型不需要高准确率）
解释：（集成学习的威力：子模型不需要太高准确率）
    1个子模型，准确率51%
    3个子模型，51.5%
    500，65.5%
    1，60%；500 99.99%

如何创建差异性？只看一部分
 放回取样（bagging或bootstrap）常用
 不放回取样（pasting）

非参数学习（决策树）更能产生差异性较大的子模型
from sklearn.ensemble import BaggingClassifier
a=BaggingClassifier(DecisionTreeClassifier(),
n_estimators=,
max_samples=,bootstrap=True，针对样本随机取样
oob_score=True,
n_jobs=-1，
max_features=,bootstrap_features=True，针对特征，随机取样
)
a.fit(X,y)oob，不分训练集测试集

bagging可能导致部分样本始终取不到
（大概有37%的样本娶不到 ）out of bag(OOB)
所以，集成学习不需要测试集，用OOB数据测试
a.oob_score_

bagging极易并行化处理n_jobs

其他产生差异化方式（上面有）
1.针对特征随机取样random subspaces（样本特征多，如人脸识别  ）
2.即针对样本，有对特征取样random patches


1.随机森林
随机森林默认每颗 决策树在节点划分上，在 随机特征子集上寻找最优划分特征（）增加差异性
随机森林使用放回取样
from sklearn.ensemble import RandomForestClassifier
RandomForestClassifier(n_estimators=,
oob_score=True
n_jobs=,)

2.Extra-Trees
决策树使用随机特征和随机阈值（差异性更大）
抑制过拟合，但增大了偏差；比随机森林更快的训练速度
from sklearn.ensemble import ExtraTreesClassifier
(n_estimators,bootstrap,oob_score)

集成学习解决回归问题
BaggingRegressor\RandomForestRegressor\ExtraTreeRegresssor


Boosting
集成多个模型，每个模型增强整体效果
3.Aba Boosting(回归，分类)
思路：每个子模型都在弥补上个模型所犯的错误，子模型投票
    回归：先生成模型，然后对不匹配模型的样本点权值加大，再生成模型，循环
提升方法没有oob数据，需要分训练集测试集
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
AdaBoostClassifier(
DecisionTreeClassifier(),
n_estimators=,
)

4.Gradient Boosting
针对错误训练模型（打高尔夫）
预测结果：模型叠加
这个算法就是以决策树为基础的，不用传估计器
from sklearn.ensemble import GradientBoostingClassifier
GradientBoostingClassifier(max_depth=,n_estimators=)

boosting 解决回归问题（）

5.Stacking（像神经网络 ）
把子模型算出的结果打包，训练出一个新的模型，输出为结果
思路：把训练集分两份，
        一份训练众多子模型，
        另一份数据分别用子模型得到结果，把结果与真值打包，训练出新的模型
训练集分几份，就有几层

最终章：学习sklearn文档
documentation--user guide书
             --API接口用法


朴素贝叶斯算法
从甲箱里取到白球概率最大，按经验，白球最有可能取自 甲箱
最大似然原理（概率最大原理）：按经验，最有可能就是最大似然
离散型随机变量最大似然法：
    1.样本观测出现概率（每个样本出现概率乘积）
        似然函数（L（θ））=概率函数（求其最大值）
    2.两边同时取对数，求导=0（灵活，也可能不用）
    3.求出θ 的估计值

连续型随机变量


全概率公式与贝叶斯公式
1.A-（C,D,E）-B          全概率公式（每条分支之和）
2.到达B，从哪来（C、D、E）--贝叶斯公式（其中一条分支占总分支比列）

全概率公式：知因推果
贝叶斯公式：知果推因，
先验概率：抽样之前有关统计问题的信息，主要来源于经验，历史资料；加工，获得分布（先验分布）
总体信息 （总体分布）、样本信息（样本观测信息）、先验分布，得到后验概率
是否利用先验信息
总体依赖于参数θ的概率密度函数
    频率派P(x;θ)
    贝叶斯派P(x|θ)

共轭先验分布的概念
定义：先验分布与后验分布同属于一个分布族

假定m个类别，预测x最大后验概率对应类别
求出文章所属各类别的概率，取最大值分类
AB不相容（互斥），AB不能同时发生
AB独立：概率不影响
联合概率p（AB）
朴素：特征独立，词之前没关系
训练文档，文档类别分布，各类别重要词频
知道文档词频统计，判断新文档（出现词）所属类别
如果某词出现0次，概率就为0；不合理，
 拉普拉斯平滑系数
p(f1/c)=(ni+a）/(n+am)m为训练文档中  特征词类别个数;a一般取1

from sklearn.naive_bayes.MultinomialNB(alpha=1.0)
朴素贝叶斯无超参数可调；所以训练集非常重要，对准确率影响非常大
先特征抽取，对每篇文章词频重要性统计；在进行朴素贝叶斯预测


优点：对缺失数据不敏感、分类准确度高，速度快
缺点：假设样本特征独立，样本特征有关联时效果不好

神经网络



非监督学习
主成分分析（PCA）
主要用于数据降维；其次可视化，去噪
如何找到让样本间间距最大的轴？
    如何定义样本间间距？方差
思路：找到一个轴，使样本映射后，方差最大
第一步：样本均值归0（相当于把坐标轴移动，方差公式化简）

目标函数：var=∑||X||^2/m,映射后，样本模的平方和最大，X映射后
向量点乘：模乘*cosθ
轴=方向向量w，模=1
向量*方向向量=向量映射到方向向量上的长度
目标函数化简为：f=var=∑(X1*w1+X2*w2 ……)^2/m 某X1样本的特征1
        求w
使用梯度上升法（搜索策略），也可以使用数学方法求解出公式

线性回归目标函数：MES，真实-预测
PCA目标函数：var，样本真实数据方差
梯度下降目标函数J，梯度上升f
关于∑的求导，就当∑不存在
PCA 梯度上升 初始w不能为0向量
PCA 不能对数据标准化
轴w 就是主成分
数据进行改变，将数据去掉第一主成分，变成新的向量，重新求第一主成分
X-X.dot(w).reshape(-1,1)*w
第一，第二，第三····主成分
主成分顺序表示重要程度 （第一，第二，第三·····）
虽然我们求出了多个主成分，但是我们的数据没有降维

降维
k个主成分 Wk矩阵，样本分别乘以k个主成分，得到k个数；样本映射到Wk的k维向量
n维映射到k维

from sklearn.decomposition import PCA（0.95）
fit求出主成分
transform降维

pca.explained_variance_radio_表示每个轴解释%多少数据方差，该功能已被封装0.95

从官方网站下载数据集
from sklearn.datesets import fetch_mldata
mnist=fetch_mldate('MNIST original')
机器学习中的数据为float
特征量纲一样，不需要标准化，如手写数字mnist数据集，一张图片的所有像素都是特征， 量纲一样

降维后升维，数据丢失，也可能是降噪（去除坏点）

特征脸就是把主成分当作样本特征来看

k-means(聚类)
假设分为k类别
1.随机取k样本，当作类别的中心
2.分别计算其余点至它们的距离，取最短作为类别，形成k个 族群
3.计算族群平均值，若与旧中心点相同，结束聚类；
                若不同，当作新中心点，重复第2，3步
from sklearn.cluster import KMeans
KMeans(n_cluster,)

KMeans评估标准
轮廓系数=（bi-ai）/max(bi,ai)[-1,1],1最好，-1最差；1代表内聚度和分离度都相对较优
bi：样本点至其它族群样本点距离平均值，取最小值
ai：样本至本类别其他样本点距离平均值
轮廓系数超过0聚类就相当不错了
from sklearn.metrics import silhouette_score(X,labels)
缺点：容易收敛到局部最优解（多次聚类）



 特征工程